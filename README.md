# ML_in_100_days


MACHINE LEARNING

1. BASIC INTRODUCTION

    1. What is the difference between Artificial Intelligence, Machine Learning and Deep Learning?

    2. What is the difference between Data Mining and Machine learning?

    3. Overfitting,underfitting ?

2. ESSENTIAL LIBRARIES THE BASICS (SIMPLE SYNTAXES, BASIC FUNCTIONS)

    4.  Pandas, Numpy , Scipy

        1. [https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)

        2. [https://docs.scipy.org/doc/scipy/reference/tutorial/index.html](https://docs.scipy.org/doc/scipy/reference/tutorial/index.html)

        3. [https://docs.scipy.org/doc/numpy/user/quickstart.html](https://docs.scipy.org/doc/numpy/user/quickstart.html)

3. DATA PREPROCESSING

    5. Importing

    6. Handling missing values.

    7. Standardizing (Categorical values, Dates , Labels)

    8. PCA

    9. Splitting

4. DATA VISUALIZATION

    10. Intro to Visualization libraries (Seaborn, Pyplot, Matplotlib)

    11. Which plot explains what ?

    12. Visualizing numerical values

    13. Visualizing non-numerical values

    14. EDA

5. SUPERVISED LEARNING**

    15. Regression

        4. Linear Regression

        5. Multi Linear Regression

        6. Weighted Linear Regression

    16. Classification

        7. Logistic Regression, Perceptron

    17. KL-divergence 

    18. Cross-entropy

    19. Natural gradient

    20. Exponential Family and Generalized Linear Models

    21. Generative Models (Gaussian Discriminant Analysis, Naive Bayes)

    22. Kernel Method (SVM, Gaussian Processes)

    23. Tree Ensembles (Decision trees, Random Forests, Boosting and Gradient Boosting)

    24. Model Selection Procedures

6. LEARNING THEORY***

    25. Feature Selection

    26. Regularization (Ridge,LASSO)

    27. Bias-Variance Decomposition and Tradeoff

    28. Concentration Inequalities

    29. Generalization and Uniform Convergence

    30. VC-dimension

7. DEEP LEARNING

    31. Neural Networks

    32. Backpropagation

    33. Deep Architectures

8. UNSUPERVISED LEARNING

    34. K-means

    35. Gaussian Mixture Model (GMM)

    36. Expectation Maximization (EM)

    37. Variational Auto-encoder (VAE)

    38. Factor Analysis

    39. Principal Components Analysis (PCA)****

    40. Independent Components Analysis (ICA)

    41. Hierarchical agglomeration

9. REINFORCEMENT LEARNING

    42. Markov Decision Processes (MDP)

    43. Bellmans Equations

    44. Value Iteration and Policy Iteration

    45. Value Function Approximation

    46. Q-Learning

10. EVALUATION METRICS*****

    47. AUC

    48. Precision

    49. Recall

    50. Specificity

    51. Mean absolute percentage error

    52. Root mean square error

NEREYE SIKIŞTIRILICAĞI BELLİ OLMAYAN TOPICLER:

* Dimensionality reduction

* Memory Reduction

* Learning Vector Quantization (LVQ)

* Ordinary least squares

* Partial Least squares

* Kernel density Estimation

* Radial basis functions

* K-fold cross validation

* Generalized Additive Models (GAMs)

* Multivariate Adaptive Regression Splines(MARS)

* Curse of dimensionality

* No free lunch theorem

* Occams Razor

SONRASI

Deep Boltzmann Machine(DBM)

Deep Belief Networks(DBN)

NLP

**Regression ve Classification anladığım kadarıyla biraz iç içe kategorilerken karışmış olma ihtimali yüksek

*** Stanford da bahsetmiş ama biz girmeli miyiz hiç emin değilim.

****PCA Data preprocessingtede kullanılıyo gibi , belki başka bi konu altında tekrar bakılıp detayına girilir.

*****EVALUATION’ı Stanford sona atmış ama biz superviseddan sonraya mı alsak? 
